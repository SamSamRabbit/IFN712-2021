{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2b10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# define stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# count vectorizer library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# script files\n",
    "from readfile import readTopic, readTopic2, readDocument, readDocument2\n",
    "from lda import buildLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3575a638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -6454.97016478401\n",
      "Model Perplexity:  397.5721552141004\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -59129.519064912645\n",
      "Model Perplexity:  111.8182846532605\n",
      "user:  101         accuracy:  0.7590987868284229\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -71625.10000504139\n",
      "Model Perplexity:  325.34321357113714\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -87160.74363242253\n",
      "Model Perplexity:  144.86540994149468\n",
      "user:  102         accuracy:  0.7337662337662337\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -16830.44254859933\n",
      "Model Perplexity:  421.35117351850494\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -116261.91947439252\n",
      "Model Perplexity:  195.303094076658\n",
      "user:  103         accuracy:  0.4772727272727273\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -66199.39967020882\n",
      "Model Perplexity:  218.7732944346546\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -75876.55878247255\n",
      "Model Perplexity:  123.5549670746386\n",
      "user:  104         accuracy:  0.6057347670250897\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -27399.30449685586\n",
      "Model Perplexity:  82.6891460744509\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -37465.53182476251\n",
      "Model Perplexity:  77.67660634339843\n",
      "user:  105         accuracy:  0.8023255813953488\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -11823.344746974082\n",
      "Model Perplexity:  340.5144505864271\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -73326.78436209327\n",
      "Model Perplexity:  172.34002091129642\n",
      "user:  106         accuracy:  0.7445482866043613\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -29544.88948140666\n",
      "Model Perplexity:  402.7604377964801\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -105976.65017771666\n",
      "Model Perplexity:  117.0734680928623\n",
      "user:  107         accuracy:  0.4956217162872154\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -13757.814216431458\n",
      "Model Perplexity:  641.2894413702554\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -54627.496179173584\n",
      "Model Perplexity:  244.69955034634697\n",
      "user:  108         accuracy:  0.5569948186528497\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -13609.911545542296\n",
      "Model Perplexity:  606.3346352673746\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -43935.8291172554\n",
      "Model Perplexity:  335.28436492816803\n",
      "user:  109         accuracy:  0.5\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -28859.222650540898\n",
      "Model Perplexity:  453.94726915449166\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -113194.56684446285\n",
      "Model Perplexity:  323.7378375038584\n",
      "user:  110         accuracy:  0.8492871690427699\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -16049.439255113288\n",
      "Model Perplexity:  246.23775295810793\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -62117.308073745575\n",
      "Model Perplexity:  142.79364636084085\n",
      "user:  111         accuracy:  0.7317073170731707\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -34822.705837942754\n",
      "Model Perplexity:  62.700298149513806\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -79579.82762650897\n",
      "Model Perplexity:  75.38319226364197\n",
      "user:  112         accuracy:  0.1995841995841996\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -16991.146073674634\n",
      "Model Perplexity:  507.5024570458593\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -95825.58177809119\n",
      "Model Perplexity:  205.46544674628947\n",
      "user:  113         accuracy:  0.644927536231884\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -8240.487989697049\n",
      "Model Perplexity:  567.5842052133356\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -55040.88912439854\n",
      "Model Perplexity:  149.84954359163197\n",
      "user:  114         accuracy:  0.6565096952908587\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -19346.660666455755\n",
      "Model Perplexity:  249.28807486010083\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -82067.68794116584\n",
      "Model Perplexity:  160.1871777014416\n",
      "user:  115         accuracy:  0.8319327731092437\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -10556.480728238834\n",
      "Model Perplexity:  727.2437707634186\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -38099.69410830011\n",
      "Model Perplexity:  226.41086283093674\n",
      "user:  116         accuracy:  0.674496644295302\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2042.4262811317326\n",
      "Model Perplexity:  295.9728096499738\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -16423.306048573948\n",
      "Model Perplexity:  21.320682538711257\n",
      "user:  117         accuracy:  0.8181818181818182\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -7917.731539319035\n",
      "Model Perplexity:  566.8082327080774\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -42344.95769270869\n",
      "Model Perplexity:  94.07420691590607\n",
      "user:  118         accuracy:  0.8907849829351536\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -6117.078049359509\n",
      "Model Perplexity:  165.08174590723547\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -35294.328370565934\n",
      "Model Perplexity:  101.13107928693056\n",
      "user:  119         accuracy:  0.7343173431734318\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -12121.327008216447\n",
      "Model Perplexity:  575.4128010376827\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -49429.61221314049\n",
      "Model Perplexity:  159.49696644559856\n",
      "user:  120         accuracy:  0.6987951807228916\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -28093.11910456485\n",
      "Model Perplexity:  154.97669497758864\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -242434.43104924046\n",
      "Model Perplexity:  80.04370759051147\n",
      "user:  121         accuracy:  0.8123953098827471\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -21153.48186878987\n",
      "Model Perplexity:  517.5407105085694\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -59536.67999834507\n",
      "Model Perplexity:  294.9439832305395\n",
      "user:  122         accuracy:  0.821882951653944\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -14585.016440801366\n",
      "Model Perplexity:  544.0111377374079\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -52001.71705679778\n",
      "Model Perplexity:  154.3532138451602\n",
      "user:  123         accuracy:  0.7923976608187134\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -15507.96403822347\n",
      "Model Perplexity:  631.3581066066232\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -68717.24858196554\n",
      "Model Perplexity:  429.73717049319373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:  124         accuracy:  0.468\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -10278.51886964253\n",
      "Model Perplexity:  484.10519413066083\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -84930.91887101243\n",
      "Model Perplexity:  219.1795107678316\n",
      "user:  125         accuracy:  0.6911764705882353\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4978.3509202933055\n",
      "Model Perplexity:  393.12216705626315\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -28574.15052137032\n",
      "Model Perplexity:  33.25547512753841\n",
      "user:  126         accuracy:  0.4888888888888889\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -9271.869979975954\n",
      "Model Perplexity:  547.5041120215587\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -27608.05795651317\n",
      "Model Perplexity:  279.20864663445013\n",
      "user:  127         accuracy:  0.40756302521008403\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -15766.860200759325\n",
      "Model Perplexity:  437.2870556964932\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -52313.9644240445\n",
      "Model Perplexity:  85.70270512322031\n",
      "user:  128         accuracy:  0.7246376811594203\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -19773.369262742042\n",
      "Model Perplexity:  559.9876376975186\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -93612.24684407059\n",
      "Model Perplexity:  236.31643303684896\n",
      "user:  129         accuracy:  0.6568047337278107\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -10362.20632472299\n",
      "Model Perplexity:  216.1481808960819\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -54314.49696676592\n",
      "Model Perplexity:  86.97220262020008\n",
      "user:  130         accuracy:  0.7654723127035831\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -22651.50475517677\n",
      "Model Perplexity:  80.5158706758843\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -94415.36273070693\n",
      "Model Perplexity:  59.73135268633381\n",
      "user:  131         accuracy:  0.6984126984126984\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -27709.291464573784\n",
      "Model Perplexity:  433.1430456047643\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -70251.56432369634\n",
      "Model Perplexity:  290.6433802887304\n",
      "user:  132         accuracy:  0.9260089686098655\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -9108.566324348743\n",
      "Model Perplexity:  619.7712290310437\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -43403.70061049647\n",
      "Model Perplexity:  238.93858208641626\n",
      "user:  133         accuracy:  0.5789473684210527\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -6130.3638319082065\n",
      "Model Perplexity:  593.3109769085775\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -39455.25432209439\n",
      "Model Perplexity:  50.13355301491101\n",
      "user:  134         accuracy:  0.6353276353276354\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -28620.482500192767\n",
      "Model Perplexity:  476.4310496782682\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -216754.82232876378\n",
      "Model Perplexity:  265.322415751783\n",
      "user:  135         accuracy:  0.8243512974051896\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -13341.645761186077\n",
      "Model Perplexity:  742.8127532856532\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -62919.10318649731\n",
      "Model Perplexity:  379.73332257501335\n",
      "user:  136         accuracy:  0.6880530973451328\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -12143.413663429388\n",
      "Model Perplexity:  506.7917076755244\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -37550.489457881326\n",
      "Model Perplexity:  329.78679018912493\n",
      "user:  137         accuracy:  0.8\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -29501.98105987886\n",
      "Model Perplexity:  491.09883323141946\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -58221.08611512553\n",
      "Model Perplexity:  201.86147816702783\n",
      "user:  138         accuracy:  0.7865853658536586\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -3723.605335389189\n",
      "Model Perplexity:  417.5788765533574\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -23776.314867709858\n",
      "Model Perplexity:  125.96250170123685\n",
      "user:  139         accuracy:  0.8853754940711462\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -17350.100329049146\n",
      "Model Perplexity:  423.7780498308659\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -67577.15586162449\n",
      "Model Perplexity:  148.20782883765602\n",
      "user:  140         accuracy:  0.6921296296296297\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -16036.323543278131\n",
      "Model Perplexity:  564.7481725217573\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -68410.10615415886\n",
      "Model Perplexity:  183.88830379761973\n",
      "user:  141         accuracy:  0.6965699208443272\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -3550.5791664470526\n",
      "Model Perplexity:  328.0449145612076\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -17287.23877406654\n",
      "Model Perplexity:  17.03783846138621\n",
      "user:  142         accuracy:  0.803030303030303\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -18231.501820408397\n",
      "Model Perplexity:  493.3820644243269\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -71280.33025488551\n",
      "Model Perplexity:  325.28095693435876\n",
      "user:  143         accuracy:  0.6258992805755396\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -11541.256306701653\n",
      "Model Perplexity:  500.8538365448206\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -34114.343726343446\n",
      "Model Perplexity:  207.6846187342389\n",
      "user:  144         accuracy:  0.5763157894736842\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -22277.349403766886\n",
      "Model Perplexity:  407.7058181851669\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -66597.87521785745\n",
      "Model Perplexity:  324.80692303193103\n",
      "user:  145         accuracy:  0.5286885245901639\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -8318.16649733893\n",
      "Model Perplexity:  493.05563578091477\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -38239.26381473242\n",
      "Model Perplexity:  270.0093862718139\n",
      "user:  146         accuracy:  0.5357142857142857\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -18221.099738818204\n",
      "Model Perplexity:  501.33261106575344\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -75710.03904744054\n",
      "Model Perplexity:  162.14603702325837\n",
      "user:  147         accuracy:  0.4368421052631579\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -8275.390688213385\n",
      "Model Perplexity:  389.1433855735883\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -52818.81167780965\n",
      "Model Perplexity:  96.65874998836544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:  148         accuracy:  0.5105263157894737\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -5762.4831834708\n",
      "Model Perplexity:  529.9690024290861\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -59148.558754233956\n",
      "Model Perplexity:  162.0349400679782\n",
      "user:  149         accuracy:  0.7906458797327395\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -16831.24446341666\n",
      "Model Perplexity:  468.75907987544355\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -61262.64020101448\n",
      "Model Perplexity:  159.51233662469645\n",
      "user:  150         accuracy:  0.6846361185983828\n"
     ]
    }
   ],
   "source": [
    "train_size = []\n",
    "test_size = []\n",
    "train_log_likelihood = []\n",
    "train_perplexity = []\n",
    "test_log_likelihood = []\n",
    "test_perplexity = []\n",
    "final_result = []\n",
    "final_result_accuracy = []\n",
    "final_result2 = []\n",
    "final_result2_accuracy = []\n",
    "perplexity = []\n",
    "log_likelihood = []\n",
    "\n",
    "for x in range(101, 151):\n",
    "    # read training data\n",
    "    num = x\n",
    "    trainSet = readTopic(num)\n",
    "    train_docs = []\n",
    "    train_relevant = []\n",
    "\n",
    "    for set in trainSet:\n",
    "        lines = readDocument(num, set[0])\n",
    "        train_docs.append(lines)\n",
    "        train_relevant.append(int(set[1]))\n",
    "    \n",
    "    train_size.append(len(train_docs))\n",
    "    \n",
    "    # read testing data\n",
    "    testSet = readTopic2(num)\n",
    "    test_docs = []\n",
    "    test_relevant = []\n",
    "\n",
    "    for set in testSet:\n",
    "        lines = readDocument2(num, set[0])\n",
    "        test_docs.append(lines)\n",
    "        test_relevant.append(int(set[1]))\n",
    "\n",
    "    test_size.append(len(test_docs))    \n",
    "        \n",
    "    data = {'documents': train_docs, 'relevant': train_relevant}\n",
    "    df = pd.DataFrame(data = data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # NLTK Stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # build count vectorizer model\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words=stop_words)\n",
    "\n",
    "    # fit and transform training data\n",
    "    count = vectorizer.fit_transform(train_docs)\n",
    "    count_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # build LDA model\n",
    "    train_lda_output, train_df_document_topics, train_df, log_likelihood, perplexity = buildLDA(count, train_docs, train_relevant)\n",
    "\n",
    "    train_log_likelihood.append(log_likelihood)\n",
    "    train_perplexity.append(perplexity)\n",
    "\n",
    "\n",
    "\n",
    "    # build count vectorizer model\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words=stop_words)\n",
    "\n",
    "    # fit training data\n",
    "    count = vectorizer.fit(train_docs)\n",
    "\n",
    "    # transform testing data\n",
    "    count = vectorizer.transform(test_docs)\n",
    "    count_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # build LDA model\n",
    "    test_lda_output, test_df_document_topics, test_df, log_likelihood, perplexity = buildLDA(count, test_docs, test_relevant)\n",
    "\n",
    "    test_log_likelihood.append(log_likelihood)\n",
    "    test_perplexity.append(perplexity)\n",
    "\n",
    "\n",
    "\n",
    "    # calculate the percentage of relevant of each topic\n",
    "    relevant_counts = []\n",
    "    irrelevant_counts = []\n",
    "    percentages = []\n",
    "    topics = []\n",
    "\n",
    "    for x in range(0, len(train_df.columns)-2):\n",
    "        relevant_counts.append(0)\n",
    "        irrelevant_counts.append(0)\n",
    "\n",
    "    for x in range(0, len(train_df)):\n",
    "        if train_df['isRelevant'][x] == 1:\n",
    "            relevant_counts[train_df['dominant_topic'][x]] += 1\n",
    "        else:\n",
    "            irrelevant_counts[train_df['dominant_topic'][x]] += 1\n",
    "\n",
    "    for x in range(0, len(relevant_counts)):\n",
    "        if (relevant_counts[x] > 0) and (irrelevant_counts[x] > 0):\n",
    "            percentage = relevant_counts[x] / (relevant_counts[x] + irrelevant_counts[x])\n",
    "            percentages.append(percentage)\n",
    "            if (percentage > 0.5):\n",
    "                topics.append(x)\n",
    "\n",
    "    \n",
    "    # if no topic is more than 50%, use the highest % topic\n",
    "    if topics == []:\n",
    "        max_value = max(relevant_counts)\n",
    "        topics.append(relevant_counts.index(max_value))\n",
    "    \n",
    "\n",
    "\n",
    "    # generate the result of testing data\n",
    "    result = []\n",
    "\n",
    "    for x in range(0, len(test_df)):\n",
    "        if test_df['dominant_topic'][x] in topics:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_result = pd.DataFrame()\n",
    "    test_result['relevant'] = test_df['isRelevant']\n",
    "    test_result['countvectorizer'] = result\n",
    "\n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    for x in range(0, len(test_result)):\n",
    "        if test_result['relevant'][x] == test_result['countvectorizer'][x]:\n",
    "            correct += 1\n",
    "    print(\"user: \", num, \"        accuracy: \", correct/len(test_result))\n",
    "    final_result.append(correct)\n",
    "    final_result_accuracy.append(correct/len(test_result))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get score(d) for each document\n",
    "    topic_col = []\n",
    "    for x in topics:\n",
    "        topic_col.append('Topic' + str(x))\n",
    "\n",
    "    scores = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        score = 0\n",
    "        for x in topic_col:\n",
    "            score += row[x]\n",
    "        scores.append(score)\n",
    "\n",
    "    test_df['score(d)'] = scores\n",
    "    df = test_df.sort_values(by=['score(d)'], ascending=False)\n",
    "    df = df.head(10)\n",
    "    final_result2.append((df.isRelevant.values == 1).sum())\n",
    "    final_result2_accuracy.append((df.isRelevant.values == 1).sum() * 0.1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b85e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_dataset_size</th>\n",
       "      <th>test_dataset_size</th>\n",
       "      <th>total_correct</th>\n",
       "      <th>total_accuracy</th>\n",
       "      <th>top10_correct</th>\n",
       "      <th>top10_accuracy</th>\n",
       "      <th>train_log_likelihood</th>\n",
       "      <th>train_perplexity</th>\n",
       "      <th>test_log_likelihood</th>\n",
       "      <th>test_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>23</td>\n",
       "      <td>577</td>\n",
       "      <td>438</td>\n",
       "      <td>0.759099</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-6454.970165</td>\n",
       "      <td>397.572155</td>\n",
       "      <td>-59129.519065</td>\n",
       "      <td>111.818285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>199</td>\n",
       "      <td>308</td>\n",
       "      <td>226</td>\n",
       "      <td>0.733766</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-71625.100005</td>\n",
       "      <td>325.343214</td>\n",
       "      <td>-87160.743632</td>\n",
       "      <td>144.865410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>64</td>\n",
       "      <td>528</td>\n",
       "      <td>252</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-16830.442549</td>\n",
       "      <td>421.351174</td>\n",
       "      <td>-116261.919474</td>\n",
       "      <td>195.303094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>194</td>\n",
       "      <td>279</td>\n",
       "      <td>169</td>\n",
       "      <td>0.605735</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-66199.399670</td>\n",
       "      <td>218.773294</td>\n",
       "      <td>-75876.558782</td>\n",
       "      <td>123.554967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>37</td>\n",
       "      <td>258</td>\n",
       "      <td>207</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-27399.304497</td>\n",
       "      <td>82.689146</td>\n",
       "      <td>-37465.531825</td>\n",
       "      <td>77.676606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>44</td>\n",
       "      <td>321</td>\n",
       "      <td>239</td>\n",
       "      <td>0.744548</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11823.344747</td>\n",
       "      <td>340.514451</td>\n",
       "      <td>-73326.784362</td>\n",
       "      <td>172.340021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>61</td>\n",
       "      <td>571</td>\n",
       "      <td>283</td>\n",
       "      <td>0.495622</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-29544.889481</td>\n",
       "      <td>402.760438</td>\n",
       "      <td>-105976.650178</td>\n",
       "      <td>117.073468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>53</td>\n",
       "      <td>386</td>\n",
       "      <td>215</td>\n",
       "      <td>0.556995</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13757.814216</td>\n",
       "      <td>641.289441</td>\n",
       "      <td>-54627.496179</td>\n",
       "      <td>244.699550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>40</td>\n",
       "      <td>240</td>\n",
       "      <td>120</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13609.911546</td>\n",
       "      <td>606.334635</td>\n",
       "      <td>-43935.829117</td>\n",
       "      <td>335.284365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>91</td>\n",
       "      <td>491</td>\n",
       "      <td>417</td>\n",
       "      <td>0.849287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-28859.222651</td>\n",
       "      <td>453.947269</td>\n",
       "      <td>-113194.566844</td>\n",
       "      <td>323.737838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>52</td>\n",
       "      <td>451</td>\n",
       "      <td>330</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-16049.439255</td>\n",
       "      <td>246.237753</td>\n",
       "      <td>-62117.308074</td>\n",
       "      <td>142.793646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>57</td>\n",
       "      <td>481</td>\n",
       "      <td>96</td>\n",
       "      <td>0.199584</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-34822.705838</td>\n",
       "      <td>62.700298</td>\n",
       "      <td>-79579.827627</td>\n",
       "      <td>75.383192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>68</td>\n",
       "      <td>552</td>\n",
       "      <td>356</td>\n",
       "      <td>0.644928</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16991.146074</td>\n",
       "      <td>507.502457</td>\n",
       "      <td>-95825.581778</td>\n",
       "      <td>205.465447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>25</td>\n",
       "      <td>361</td>\n",
       "      <td>237</td>\n",
       "      <td>0.656510</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8240.487990</td>\n",
       "      <td>567.584205</td>\n",
       "      <td>-55040.889124</td>\n",
       "      <td>149.849544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>46</td>\n",
       "      <td>357</td>\n",
       "      <td>297</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-19346.660666</td>\n",
       "      <td>249.288075</td>\n",
       "      <td>-82067.687941</td>\n",
       "      <td>160.187178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>46</td>\n",
       "      <td>298</td>\n",
       "      <td>201</td>\n",
       "      <td>0.674497</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-10556.480728</td>\n",
       "      <td>727.243771</td>\n",
       "      <td>-38099.694108</td>\n",
       "      <td>226.410863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>13</td>\n",
       "      <td>297</td>\n",
       "      <td>243</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-2042.426281</td>\n",
       "      <td>295.972810</td>\n",
       "      <td>-16423.306049</td>\n",
       "      <td>21.320683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>32</td>\n",
       "      <td>293</td>\n",
       "      <td>261</td>\n",
       "      <td>0.890785</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7917.731539</td>\n",
       "      <td>566.808233</td>\n",
       "      <td>-42344.957693</td>\n",
       "      <td>94.074207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>26</td>\n",
       "      <td>271</td>\n",
       "      <td>199</td>\n",
       "      <td>0.734317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6117.078049</td>\n",
       "      <td>165.081746</td>\n",
       "      <td>-35294.328371</td>\n",
       "      <td>101.131079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>54</td>\n",
       "      <td>415</td>\n",
       "      <td>290</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-12121.327008</td>\n",
       "      <td>575.412801</td>\n",
       "      <td>-49429.612213</td>\n",
       "      <td>159.496966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>81</td>\n",
       "      <td>597</td>\n",
       "      <td>485</td>\n",
       "      <td>0.812395</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-28093.119105</td>\n",
       "      <td>154.976695</td>\n",
       "      <td>-242434.431049</td>\n",
       "      <td>80.043708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>70</td>\n",
       "      <td>393</td>\n",
       "      <td>323</td>\n",
       "      <td>0.821883</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-21153.481869</td>\n",
       "      <td>517.540711</td>\n",
       "      <td>-59536.679998</td>\n",
       "      <td>294.943983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>51</td>\n",
       "      <td>342</td>\n",
       "      <td>271</td>\n",
       "      <td>0.792398</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14585.016441</td>\n",
       "      <td>544.011138</td>\n",
       "      <td>-52001.717057</td>\n",
       "      <td>154.353214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>33</td>\n",
       "      <td>250</td>\n",
       "      <td>117</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15507.964038</td>\n",
       "      <td>631.358107</td>\n",
       "      <td>-68717.248582</td>\n",
       "      <td>429.737170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>36</td>\n",
       "      <td>544</td>\n",
       "      <td>376</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10278.518870</td>\n",
       "      <td>484.105194</td>\n",
       "      <td>-84930.918871</td>\n",
       "      <td>219.179511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>29</td>\n",
       "      <td>270</td>\n",
       "      <td>132</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-4978.350920</td>\n",
       "      <td>393.122167</td>\n",
       "      <td>-28574.150521</td>\n",
       "      <td>33.255475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>32</td>\n",
       "      <td>238</td>\n",
       "      <td>97</td>\n",
       "      <td>0.407563</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9271.869980</td>\n",
       "      <td>547.504112</td>\n",
       "      <td>-27608.057957</td>\n",
       "      <td>279.208647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>51</td>\n",
       "      <td>276</td>\n",
       "      <td>200</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15766.860201</td>\n",
       "      <td>437.287056</td>\n",
       "      <td>-52313.964424</td>\n",
       "      <td>85.702705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>72</td>\n",
       "      <td>507</td>\n",
       "      <td>333</td>\n",
       "      <td>0.656805</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19773.369263</td>\n",
       "      <td>559.987638</td>\n",
       "      <td>-93612.246844</td>\n",
       "      <td>236.316433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>24</td>\n",
       "      <td>307</td>\n",
       "      <td>235</td>\n",
       "      <td>0.765472</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10362.206325</td>\n",
       "      <td>216.148181</td>\n",
       "      <td>-54314.496967</td>\n",
       "      <td>86.972203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>31</td>\n",
       "      <td>252</td>\n",
       "      <td>176</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-22651.504755</td>\n",
       "      <td>80.515871</td>\n",
       "      <td>-94415.362731</td>\n",
       "      <td>59.731353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>103</td>\n",
       "      <td>446</td>\n",
       "      <td>413</td>\n",
       "      <td>0.926009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-27709.291465</td>\n",
       "      <td>433.143046</td>\n",
       "      <td>-70251.564324</td>\n",
       "      <td>290.643380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>47</td>\n",
       "      <td>380</td>\n",
       "      <td>220</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-9108.566324</td>\n",
       "      <td>619.771229</td>\n",
       "      <td>-43403.700610</td>\n",
       "      <td>238.938582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>31</td>\n",
       "      <td>351</td>\n",
       "      <td>223</td>\n",
       "      <td>0.635328</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6130.363832</td>\n",
       "      <td>593.310977</td>\n",
       "      <td>-39455.254322</td>\n",
       "      <td>50.133553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>29</td>\n",
       "      <td>501</td>\n",
       "      <td>413</td>\n",
       "      <td>0.824351</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-28620.482500</td>\n",
       "      <td>476.431050</td>\n",
       "      <td>-216754.822329</td>\n",
       "      <td>265.322416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>46</td>\n",
       "      <td>452</td>\n",
       "      <td>311</td>\n",
       "      <td>0.688053</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-13341.645761</td>\n",
       "      <td>742.812753</td>\n",
       "      <td>-62919.103186</td>\n",
       "      <td>379.733323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>50</td>\n",
       "      <td>325</td>\n",
       "      <td>260</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12143.413663</td>\n",
       "      <td>506.791708</td>\n",
       "      <td>-37550.489458</td>\n",
       "      <td>329.786790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>98</td>\n",
       "      <td>328</td>\n",
       "      <td>258</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-29501.981060</td>\n",
       "      <td>491.098833</td>\n",
       "      <td>-58221.086115</td>\n",
       "      <td>201.861478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>21</td>\n",
       "      <td>253</td>\n",
       "      <td>224</td>\n",
       "      <td>0.885375</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3723.605335</td>\n",
       "      <td>417.578877</td>\n",
       "      <td>-23776.314868</td>\n",
       "      <td>125.962502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>59</td>\n",
       "      <td>432</td>\n",
       "      <td>299</td>\n",
       "      <td>0.692130</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-17350.100329</td>\n",
       "      <td>423.778050</td>\n",
       "      <td>-67577.155862</td>\n",
       "      <td>148.207829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>56</td>\n",
       "      <td>379</td>\n",
       "      <td>264</td>\n",
       "      <td>0.696570</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-16036.323543</td>\n",
       "      <td>564.748173</td>\n",
       "      <td>-68410.106154</td>\n",
       "      <td>183.888304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>28</td>\n",
       "      <td>198</td>\n",
       "      <td>159</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3550.579166</td>\n",
       "      <td>328.044915</td>\n",
       "      <td>-17287.238774</td>\n",
       "      <td>17.037838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>52</td>\n",
       "      <td>417</td>\n",
       "      <td>261</td>\n",
       "      <td>0.625899</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18231.501820</td>\n",
       "      <td>493.382064</td>\n",
       "      <td>-71280.330255</td>\n",
       "      <td>325.280957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>50</td>\n",
       "      <td>380</td>\n",
       "      <td>219</td>\n",
       "      <td>0.576316</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11541.256307</td>\n",
       "      <td>500.853837</td>\n",
       "      <td>-34114.343726</td>\n",
       "      <td>207.684619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>95</td>\n",
       "      <td>488</td>\n",
       "      <td>258</td>\n",
       "      <td>0.528689</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-22277.349404</td>\n",
       "      <td>407.705818</td>\n",
       "      <td>-66597.875218</td>\n",
       "      <td>324.806923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>32</td>\n",
       "      <td>280</td>\n",
       "      <td>150</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-8318.166497</td>\n",
       "      <td>493.055636</td>\n",
       "      <td>-38239.263815</td>\n",
       "      <td>270.009386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>62</td>\n",
       "      <td>380</td>\n",
       "      <td>166</td>\n",
       "      <td>0.436842</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18221.099739</td>\n",
       "      <td>501.332611</td>\n",
       "      <td>-75710.039047</td>\n",
       "      <td>162.146037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>33</td>\n",
       "      <td>380</td>\n",
       "      <td>194</td>\n",
       "      <td>0.510526</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-8275.390688</td>\n",
       "      <td>389.143386</td>\n",
       "      <td>-52818.811678</td>\n",
       "      <td>96.658750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>26</td>\n",
       "      <td>449</td>\n",
       "      <td>355</td>\n",
       "      <td>0.790646</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5762.483183</td>\n",
       "      <td>529.969002</td>\n",
       "      <td>-59148.558754</td>\n",
       "      <td>162.034940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>51</td>\n",
       "      <td>371</td>\n",
       "      <td>254</td>\n",
       "      <td>0.684636</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16831.244463</td>\n",
       "      <td>468.759080</td>\n",
       "      <td>-61262.640201</td>\n",
       "      <td>159.512337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_dataset_size  test_dataset_size  total_correct  total_accuracy  \\\n",
       "101                  23                577            438        0.759099   \n",
       "102                 199                308            226        0.733766   \n",
       "103                  64                528            252        0.477273   \n",
       "104                 194                279            169        0.605735   \n",
       "105                  37                258            207        0.802326   \n",
       "106                  44                321            239        0.744548   \n",
       "107                  61                571            283        0.495622   \n",
       "108                  53                386            215        0.556995   \n",
       "109                  40                240            120        0.500000   \n",
       "110                  91                491            417        0.849287   \n",
       "111                  52                451            330        0.731707   \n",
       "112                  57                481             96        0.199584   \n",
       "113                  68                552            356        0.644928   \n",
       "114                  25                361            237        0.656510   \n",
       "115                  46                357            297        0.831933   \n",
       "116                  46                298            201        0.674497   \n",
       "117                  13                297            243        0.818182   \n",
       "118                  32                293            261        0.890785   \n",
       "119                  26                271            199        0.734317   \n",
       "120                  54                415            290        0.698795   \n",
       "121                  81                597            485        0.812395   \n",
       "122                  70                393            323        0.821883   \n",
       "123                  51                342            271        0.792398   \n",
       "124                  33                250            117        0.468000   \n",
       "125                  36                544            376        0.691176   \n",
       "126                  29                270            132        0.488889   \n",
       "127                  32                238             97        0.407563   \n",
       "128                  51                276            200        0.724638   \n",
       "129                  72                507            333        0.656805   \n",
       "130                  24                307            235        0.765472   \n",
       "131                  31                252            176        0.698413   \n",
       "132                 103                446            413        0.926009   \n",
       "133                  47                380            220        0.578947   \n",
       "134                  31                351            223        0.635328   \n",
       "135                  29                501            413        0.824351   \n",
       "136                  46                452            311        0.688053   \n",
       "137                  50                325            260        0.800000   \n",
       "138                  98                328            258        0.786585   \n",
       "139                  21                253            224        0.885375   \n",
       "140                  59                432            299        0.692130   \n",
       "141                  56                379            264        0.696570   \n",
       "142                  28                198            159        0.803030   \n",
       "143                  52                417            261        0.625899   \n",
       "144                  50                380            219        0.576316   \n",
       "145                  95                488            258        0.528689   \n",
       "146                  32                280            150        0.535714   \n",
       "147                  62                380            166        0.436842   \n",
       "148                  33                380            194        0.510526   \n",
       "149                  26                449            355        0.790646   \n",
       "150                  51                371            254        0.684636   \n",
       "\n",
       "     top10_correct  top10_accuracy  train_log_likelihood  train_perplexity  \\\n",
       "101              8             0.8          -6454.970165        397.572155   \n",
       "102              7             0.7         -71625.100005        325.343214   \n",
       "103              1             0.1         -16830.442549        421.351174   \n",
       "104              7             0.7         -66199.399670        218.773294   \n",
       "105              5             0.5         -27399.304497         82.689146   \n",
       "106              0             0.0         -11823.344747        340.514451   \n",
       "107              0             0.0         -29544.889481        402.760438   \n",
       "108              0             0.0         -13757.814216        641.289441   \n",
       "109              0             0.0         -13609.911546        606.334635   \n",
       "110              0             0.0         -28859.222651        453.947269   \n",
       "111              1             0.1         -16049.439255        246.237753   \n",
       "112              0             0.0         -34822.705838         62.700298   \n",
       "113              0             0.0         -16991.146074        507.502457   \n",
       "114              0             0.0          -8240.487990        567.584205   \n",
       "115              8             0.8         -19346.660666        249.288075   \n",
       "116              3             0.3         -10556.480728        727.243771   \n",
       "117              7             0.7          -2042.426281        295.972810   \n",
       "118              0             0.0          -7917.731539        566.808233   \n",
       "119              0             0.0          -6117.078049        165.081746   \n",
       "120              3             0.3         -12121.327008        575.412801   \n",
       "121              0             0.0         -28093.119105        154.976695   \n",
       "122              8             0.8         -21153.481869        517.540711   \n",
       "123              0             0.0         -14585.016441        544.011138   \n",
       "124              0             0.0         -15507.964038        631.358107   \n",
       "125              0             0.0         -10278.518870        484.105194   \n",
       "126              9             0.9          -4978.350920        393.122167   \n",
       "127              0             0.0          -9271.869980        547.504112   \n",
       "128              0             0.0         -15766.860201        437.287056   \n",
       "129              0             0.0         -19773.369263        559.987638   \n",
       "130              0             0.0         -10362.206325        216.148181   \n",
       "131              0             0.0         -22651.504755         80.515871   \n",
       "132              1             0.1         -27709.291465        433.143046   \n",
       "133              1             0.1          -9108.566324        619.771229   \n",
       "134              0             0.0          -6130.363832        593.310977   \n",
       "135              8             0.8         -28620.482500        476.431050   \n",
       "136              1             0.1         -13341.645761        742.812753   \n",
       "137              0             0.0         -12143.413663        506.791708   \n",
       "138              0             0.0         -29501.981060        491.098833   \n",
       "139              0             0.0          -3723.605335        417.578877   \n",
       "140              1             0.1         -17350.100329        423.778050   \n",
       "141              2             0.2         -16036.323543        564.748173   \n",
       "142              0             0.0          -3550.579166        328.044915   \n",
       "143              0             0.0         -18231.501820        493.382064   \n",
       "144              0             0.0         -11541.256307        500.853837   \n",
       "145              0             0.0         -22277.349404        407.705818   \n",
       "146              4             0.4          -8318.166497        493.055636   \n",
       "147              0             0.0         -18221.099739        501.332611   \n",
       "148              7             0.7          -8275.390688        389.143386   \n",
       "149              0             0.0          -5762.483183        529.969002   \n",
       "150              0             0.0         -16831.244463        468.759080   \n",
       "\n",
       "     test_log_likelihood  test_perplexity  \n",
       "101        -59129.519065       111.818285  \n",
       "102        -87160.743632       144.865410  \n",
       "103       -116261.919474       195.303094  \n",
       "104        -75876.558782       123.554967  \n",
       "105        -37465.531825        77.676606  \n",
       "106        -73326.784362       172.340021  \n",
       "107       -105976.650178       117.073468  \n",
       "108        -54627.496179       244.699550  \n",
       "109        -43935.829117       335.284365  \n",
       "110       -113194.566844       323.737838  \n",
       "111        -62117.308074       142.793646  \n",
       "112        -79579.827627        75.383192  \n",
       "113        -95825.581778       205.465447  \n",
       "114        -55040.889124       149.849544  \n",
       "115        -82067.687941       160.187178  \n",
       "116        -38099.694108       226.410863  \n",
       "117        -16423.306049        21.320683  \n",
       "118        -42344.957693        94.074207  \n",
       "119        -35294.328371       101.131079  \n",
       "120        -49429.612213       159.496966  \n",
       "121       -242434.431049        80.043708  \n",
       "122        -59536.679998       294.943983  \n",
       "123        -52001.717057       154.353214  \n",
       "124        -68717.248582       429.737170  \n",
       "125        -84930.918871       219.179511  \n",
       "126        -28574.150521        33.255475  \n",
       "127        -27608.057957       279.208647  \n",
       "128        -52313.964424        85.702705  \n",
       "129        -93612.246844       236.316433  \n",
       "130        -54314.496967        86.972203  \n",
       "131        -94415.362731        59.731353  \n",
       "132        -70251.564324       290.643380  \n",
       "133        -43403.700610       238.938582  \n",
       "134        -39455.254322        50.133553  \n",
       "135       -216754.822329       265.322416  \n",
       "136        -62919.103186       379.733323  \n",
       "137        -37550.489458       329.786790  \n",
       "138        -58221.086115       201.861478  \n",
       "139        -23776.314868       125.962502  \n",
       "140        -67577.155862       148.207829  \n",
       "141        -68410.106154       183.888304  \n",
       "142        -17287.238774        17.037838  \n",
       "143        -71280.330255       325.280957  \n",
       "144        -34114.343726       207.684619  \n",
       "145        -66597.875218       324.806923  \n",
       "146        -38239.263815       270.009386  \n",
       "147        -75710.039047       162.146037  \n",
       "148        -52818.811678        96.658750  \n",
       "149        -59148.558754       162.034940  \n",
       "150        -61262.640201       159.512337  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = []\n",
    "for x in range(101, 151):\n",
    "    users.append(x)\n",
    "    \n",
    "df = pd.DataFrame(index=users)\n",
    "df['train_dataset_size'] = train_size\n",
    "df['test_dataset_size'] = test_size\n",
    "df['total_correct'] = final_result\n",
    "df['total_accuracy'] = final_result_accuracy\n",
    "df['top10_correct'] = final_result2\n",
    "df['top10_accuracy'] = final_result2_accuracy\n",
    "df['train_log_likelihood'] = train_log_likelihood\n",
    "df['train_perplexity'] = train_perplexity\n",
    "df['test_log_likelihood'] = test_log_likelihood\n",
    "df['test_perplexity'] = test_perplexity\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ff677d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=r'C:\\Users\\sumsu\\Desktop\\QUT\\sem2\\IFN712\\project\\RCV1 Dataset'\n",
    "# df.to_csv(path+'countvectorizer_result.csv')\n",
    "df.to_csv('countvectorizer_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9de380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
