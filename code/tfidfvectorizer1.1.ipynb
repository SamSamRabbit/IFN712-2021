{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c118fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# define stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# tf-idf library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# script files\n",
    "from readfile import readTopic, readTopic2, readDocument, readDocument2\n",
    "from lda import buildLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f5a41d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -515.1693639896725\n",
      "Model Perplexity:  6615.645742402023\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -5862.518816344052\n",
      "Model Perplexity:  633.1676437451687\n",
      "user:  101         accuracy:  0.6447140381282496\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -5799.870234487241\n",
      "Model Perplexity:  8396.555199374157\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -6823.307162902565\n",
      "Model Perplexity:  5272.106012527262\n",
      "user:  102         accuracy:  0.4837662337662338\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1450.3267324527264\n",
      "Model Perplexity:  10398.113020143202\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -7246.05190583114\n",
      "Model Perplexity:  1905.8715433338075\n",
      "user:  103         accuracy:  0.3484848484848485\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -5661.168295562941\n",
      "Model Perplexity:  8149.67295996465\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -6131.230195157184\n",
      "Model Perplexity:  4777.647383218788\n",
      "user:  104         accuracy:  0.6630824372759857\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -741.7312697479723\n",
      "Model Perplexity:  6944.454356228425\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2608.1573880168553\n",
      "Model Perplexity:  900.3307314894215\n",
      "user:  105         accuracy:  0.748062015503876\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -825.5385647241089\n",
      "Model Perplexity:  5442.020840702577\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -3785.723396938172\n",
      "Model Perplexity:  1090.6699337808616\n",
      "user:  106         accuracy:  0.2616822429906542\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1410.1243734028242\n",
      "Model Perplexity:  14905.760187820364\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -7888.573989775823\n",
      "Model Perplexity:  2414.717161471333\n",
      "user:  107         accuracy:  0.2626970227670753\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1131.178560325782\n",
      "Model Perplexity:  7495.668233047646\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -5146.129776419604\n",
      "Model Perplexity:  1419.699789360931\n",
      "user:  108         accuracy:  0.24352331606217617\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1062.0970734067607\n",
      "Model Perplexity:  6777.095667262163\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4093.1248000328223\n",
      "Model Perplexity:  1857.6092834267486\n",
      "user:  109         accuracy:  0.5375\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2544.6773322823387\n",
      "Model Perplexity:  8466.793201421488\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -9718.791725883173\n",
      "Model Perplexity:  2322.317592157122\n",
      "user:  110         accuracy:  0.1079429735234216\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -996.5474534745042\n",
      "Model Perplexity:  6698.215650282244\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -5274.069872494627\n",
      "Model Perplexity:  1133.4909813689562\n",
      "user:  111         accuracy:  0.17073170731707318\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -868.2269342211639\n",
      "Model Perplexity:  3902.2839686140674\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4746.843174738884\n",
      "Model Perplexity:  729.7008020113755\n",
      "user:  112         accuracy:  0.09563409563409564\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1416.893447276931\n",
      "Model Perplexity:  7226.283412840911\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -7220.440218728368\n",
      "Model Perplexity:  1506.6846660001222\n",
      "user:  113         accuracy:  0.7844202898550725\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -558.4242671486993\n",
      "Model Perplexity:  6856.129484859824\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4622.800394532345\n",
      "Model Perplexity:  1040.6221328820059\n",
      "user:  114         accuracy:  0.8282548476454293\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -938.1617356916437\n",
      "Model Perplexity:  5174.051526840156\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4154.020431681044\n",
      "Model Perplexity:  1221.2768188169666\n",
      "user:  115         accuracy:  0.8235294117647058\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -962.8128623067296\n",
      "Model Perplexity:  7610.620555535287\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -3877.6756429246475\n",
      "Model Perplexity:  1619.4382407325709\n",
      "user:  116         accuracy:  0.5738255033557047\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -231.6810502385034\n",
      "Model Perplexity:  5602.885775994376\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2307.3669567446204\n",
      "Model Perplexity:  400.41083633345\n",
      "user:  117         accuracy:  0.8922558922558923\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -670.2858557695909\n",
      "Model Perplexity:  7428.37186694578\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -3546.3738453076526\n",
      "Model Perplexity:  1215.8878764843262\n",
      "user:  118         accuracy:  0.2150170648464164\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -492.3465879352205\n",
      "Model Perplexity:  4268.777744029721\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2881.1832918732193\n",
      "Model Perplexity:  651.7773343294119\n",
      "user:  119         accuracy:  0.2988929889298893\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1034.496283392044\n",
      "Model Perplexity:  5204.7714425761105\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4658.324935575043\n",
      "Model Perplexity:  1150.6903572792762\n",
      "user:  120         accuracy:  0.6144578313253012\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2045.2790963965913\n",
      "Model Perplexity:  8953.846476628169\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -10426.40941850041\n",
      "Model Perplexity:  2211.214272630855\n",
      "user:  121         accuracy:  0.8509212730318257\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2110.3812979288787\n",
      "Model Perplexity:  12368.483678723325\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -6858.282110847691\n",
      "Model Perplexity:  2636.5911003927213\n",
      "user:  122         accuracy:  0.1297709923664122\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1088.399688496278\n",
      "Model Perplexity:  6572.7508453466535\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4173.238518991486\n",
      "Model Perplexity:  1408.2429885903332\n",
      "user:  123         accuracy:  0.21637426900584794\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -994.3133230950114\n",
      "Model Perplexity:  12707.433166390112\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4848.891997463644\n",
      "Model Perplexity:  2327.504679765721\n",
      "user:  124         accuracy:  0.868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -756.328073117674\n",
      "Model Perplexity:  5288.014401840882\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -6307.97110566572\n",
      "Model Perplexity:  729.2844993568598\n",
      "user:  125         accuracy:  0.46875\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -549.4037592408956\n",
      "Model Perplexity:  3711.657857864897\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2766.584549797621\n",
      "Model Perplexity:  798.8477234743567\n",
      "user:  126         accuracy:  0.37777777777777777\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -726.3725642295165\n",
      "Model Perplexity:  9334.300828687543\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2877.967754824337\n",
      "Model Perplexity:  1407.5821281600797\n",
      "user:  127         accuracy:  0.8109243697478992\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1241.1116511806579\n",
      "Model Perplexity:  10058.535180825671\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4001.0842729353135\n",
      "Model Perplexity:  2267.6909261595188\n",
      "user:  128         accuracy:  0.7862318840579711\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1907.7067723432538\n",
      "Model Perplexity:  10464.71598380857\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -7914.11755532774\n",
      "Model Perplexity:  2368.045582105563\n",
      "user:  129         accuracy:  0.24260355029585798\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -456.9581336435393\n",
      "Model Perplexity:  5980.7668551986335\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2881.4570949932518\n",
      "Model Perplexity:  697.5043885119143\n",
      "user:  130         accuracy:  0.9478827361563518\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -588.1222040172513\n",
      "Model Perplexity:  8003.426068070376\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2974.514113098084\n",
      "Model Perplexity:  1026.4691420485435\n",
      "user:  131         accuracy:  0.626984126984127\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2603.3750417750543\n",
      "Model Perplexity:  6821.03745028357\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -7605.964306336362\n",
      "Model Perplexity:  2386.7590133479716\n",
      "user:  132         accuracy:  0.6188340807174888\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -924.5974509938233\n",
      "Model Perplexity:  9050.083608214352\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4537.784521186854\n",
      "Model Perplexity:  1327.90107093411\n",
      "user:  133         accuracy:  0.9263157894736842\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -664.7958986584301\n",
      "Model Perplexity:  5525.060742633172\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -3756.4064475715363\n",
      "Model Perplexity:  1031.1175597305316\n",
      "user:  134         accuracy:  0.8091168091168092\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -876.1149057612247\n",
      "Model Perplexity:  17681.508213057554\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -8399.574758498742\n",
      "Model Perplexity:  1841.7599765140083\n",
      "user:  135         accuracy:  0.8203592814371258\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1150.3219742555348\n",
      "Model Perplexity:  9563.581760818046\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -6502.14141688458\n",
      "Model Perplexity:  1802.2803977677213\n",
      "user:  136         accuracy:  0.21460176991150443\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -964.5690083325053\n",
      "Model Perplexity:  8774.595959657403\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -3839.7441893649166\n",
      "Model Perplexity:  1586.5663937034424\n",
      "user:  137         accuracy:  0.08\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2783.863952852646\n",
      "Model Perplexity:  6672.771485650144\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -5813.950280900415\n",
      "Model Perplexity:  2569.7295586653267\n",
      "user:  138         accuracy:  0.8658536585365854\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -405.09254963499797\n",
      "Model Perplexity:  7921.108455445844\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2712.0624828035316\n",
      "Model Perplexity:  802.3018008396523\n",
      "user:  139         accuracy:  0.8656126482213439\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1112.9330744731083\n",
      "Model Perplexity:  6943.217988500428\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -5055.213671349522\n",
      "Model Perplexity:  1393.9874443770016\n",
      "user:  140         accuracy:  0.35185185185185186\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1382.1688421359875\n",
      "Model Perplexity:  12749.442749439635\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -5128.681791671426\n",
      "Model Perplexity:  2008.228957841129\n",
      "user:  141         accuracy:  0.24538258575197888\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -491.88270612366597\n",
      "Model Perplexity:  3098.2933084210667\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1975.300543292796\n",
      "Model Perplexity:  729.2999736759817\n",
      "user:  142         accuracy:  0.6212121212121212\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1378.5240630182009\n",
      "Model Perplexity:  8967.764765725371\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -6376.578386967175\n",
      "Model Perplexity:  1856.5452554030724\n",
      "user:  143         accuracy:  0.10071942446043165\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1080.7587763125387\n",
      "Model Perplexity:  8639.294208307554\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4184.748741629383\n",
      "Model Perplexity:  1365.3454148660312\n",
      "user:  144         accuracy:  0.6921052631578948\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -2161.645636822635\n",
      "Model Perplexity:  6865.854095668729\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -6917.663934247672\n",
      "Model Perplexity:  2065.0906132855807\n",
      "user:  145         accuracy:  0.9098360655737705\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -711.296845981837\n",
      "Model Perplexity:  5433.975142876123\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -3738.668958743709\n",
      "Model Perplexity:  1082.893013104889\n",
      "user:  146         accuracy:  0.5392857142857143\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1457.8474697224817\n",
      "Model Perplexity:  9896.22977253514\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -5637.76480558687\n",
      "Model Perplexity:  2333.8845776942976\n",
      "user:  147         accuracy:  0.14210526315789473\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -666.2340632278115\n",
      "Model Perplexity:  5753.903379193996\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4568.531120126228\n",
      "Model Perplexity:  801.547052659345\n",
      "user:  148         accuracy:  0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -498.82881886435837\n",
      "Model Perplexity:  5654.895657560063\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4796.714638134622\n",
      "Model Perplexity:  689.8148546737955\n",
      "user:  149         accuracy:  0.1915367483296214\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1094.8318955656655\n",
      "Model Perplexity:  4699.8685555013\n",
      "Best Model's Params:  {'learning_decay': 0.3, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4631.532304369635\n",
      "Model Perplexity:  1028.7933569720876\n",
      "user:  150         accuracy:  0.3261455525606469\n"
     ]
    }
   ],
   "source": [
    "train_size = []\n",
    "test_size = []\n",
    "train_log_likelihood = []\n",
    "train_perplexity = []\n",
    "test_log_likelihood = []\n",
    "test_perplexity = []\n",
    "final_result = []\n",
    "final_result_accuracy = []\n",
    "final_result2 = []\n",
    "final_result2_accuracy = []\n",
    "perplexity = []\n",
    "log_likelihood = []\n",
    "for x in range(101, 151):\n",
    "    # read training data\n",
    "    num = x\n",
    "    trainSet = readTopic(num)\n",
    "    train_docs = []\n",
    "    train_relevant = []\n",
    "\n",
    "    for set in trainSet:\n",
    "        lines = readDocument(num, set[0])\n",
    "        train_docs.append(lines)\n",
    "        train_relevant.append(int(set[1]))\n",
    "    \n",
    "    train_size.append(len(train_docs))\n",
    "    \n",
    "    # read testing data\n",
    "    testSet = readTopic2(num)\n",
    "    test_docs = []\n",
    "    test_relevant = []\n",
    "\n",
    "    for set in testSet:\n",
    "        lines = readDocument2(num, set[0])\n",
    "        test_docs.append(lines)\n",
    "        test_relevant.append(int(set[1]))\n",
    "\n",
    "    test_size.append(len(test_docs))    \n",
    "        \n",
    "    data = {'documents': train_docs, 'relevant': train_relevant}\n",
    "    df = pd.DataFrame(data = data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # NLTK Stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # build tf-idf model\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "    \n",
    "    # fit and transform training data\n",
    "    tfidf = vectorizer.fit_transform(train_docs)\n",
    "    count_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # build LDA model\n",
    "    train_lda_output, train_df_document_topics, train_df, log_likelihood, perplexity = buildLDA(tfidf, train_docs, train_relevant)\n",
    "    \n",
    "    train_log_likelihood.append(log_likelihood)\n",
    "    train_perplexity.append(perplexity)\n",
    "\n",
    "\n",
    "\n",
    "    # build count vectorizer model\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "\n",
    "    # fit training data\n",
    "    tfidf = vectorizer.fit(train_docs)\n",
    "\n",
    "\n",
    "    # transform testing data\n",
    "    tfidf = vectorizer.transform(test_docs)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    # build LDA model\n",
    "    test_lda_output, test_df_document_topics, test_df, log_likelihood, perplexity = buildLDA(tfidf, test_docs, test_relevant)\n",
    "\n",
    "    test_log_likelihood.append(log_likelihood)\n",
    "    test_perplexity.append(perplexity)\n",
    "\n",
    "\n",
    "\n",
    "    # calculate the percentage of relevant of each topic\n",
    "    relevant_counts = []\n",
    "    irrelevant_counts = []\n",
    "    percentages = []\n",
    "    topics = []\n",
    "\n",
    "    for x in range(0, len(train_df.columns)-2):\n",
    "        relevant_counts.append(0)\n",
    "        irrelevant_counts.append(0)\n",
    "\n",
    "    for x in range(0, len(train_df)):\n",
    "        if train_df['isRelevant'][x] == 1:\n",
    "            relevant_counts[train_df['dominant_topic'][x]] += 1\n",
    "        else:\n",
    "            irrelevant_counts[train_df['dominant_topic'][x]] += 1\n",
    "\n",
    "    for x in range(0, len(relevant_counts)):\n",
    "        if (relevant_counts[x] > 0) and (irrelevant_counts[x] > 0):\n",
    "            percentage = relevant_counts[x] / (relevant_counts[x] + irrelevant_counts[x])\n",
    "            percentages.append(percentage)\n",
    "            if (percentage > 0.5):\n",
    "                topics.append(x)\n",
    "\n",
    "    \n",
    "    # if no topic is more than 50%, use the highest % topic\n",
    "    if topics == []:\n",
    "        max_value = max(relevant_counts)\n",
    "        topics.append(relevant_counts.index(max_value))\n",
    "    \n",
    "\n",
    "\n",
    "    # generate the result of testing data\n",
    "    result = []\n",
    "\n",
    "    for x in range(0, len(test_df)):\n",
    "        if test_df['dominant_topic'][x] in topics:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_result = pd.DataFrame()\n",
    "    test_result['relevant'] = test_df['isRelevant']\n",
    "    test_result['countvectorizer'] = result\n",
    "\n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    for x in range(0, len(test_result)):\n",
    "        if test_result['relevant'][x] == test_result['countvectorizer'][x]:\n",
    "            correct += 1\n",
    "    print(\"user: \", num, \"        accuracy: \", correct/len(test_result))\n",
    "    final_result.append(correct)\n",
    "    final_result_accuracy.append(correct/len(test_result))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get score(d) for each document\n",
    "    topic_col = []\n",
    "    for x in topics:\n",
    "        topic_col.append('Topic' + str(x))\n",
    "\n",
    "    scores = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        score = 0\n",
    "        for x in topic_col:\n",
    "            score += row[x]\n",
    "        scores.append(score)\n",
    "\n",
    "    test_df['score(d)'] = scores\n",
    "    df = test_df.sort_values(by=['score(d)'], ascending=False)\n",
    "    df = df.head(10)\n",
    "    final_result2.append((df.isRelevant.values == 1).sum())\n",
    "    final_result2_accuracy.append((df.isRelevant.values == 1).sum() * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d21b54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_dataset_size</th>\n",
       "      <th>test_dataset_size</th>\n",
       "      <th>total_correct</th>\n",
       "      <th>total_accuracy</th>\n",
       "      <th>top10_correct</th>\n",
       "      <th>top10_accuracy</th>\n",
       "      <th>train_log_likelihood</th>\n",
       "      <th>train_perplexity</th>\n",
       "      <th>test_log_likelihood</th>\n",
       "      <th>test_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>23</td>\n",
       "      <td>577</td>\n",
       "      <td>372</td>\n",
       "      <td>0.644714</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-515.169364</td>\n",
       "      <td>6615.645742</td>\n",
       "      <td>-5862.518816</td>\n",
       "      <td>633.167644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>199</td>\n",
       "      <td>308</td>\n",
       "      <td>149</td>\n",
       "      <td>0.483766</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5799.870234</td>\n",
       "      <td>8396.555199</td>\n",
       "      <td>-6823.307163</td>\n",
       "      <td>5272.106013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>64</td>\n",
       "      <td>528</td>\n",
       "      <td>184</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1450.326732</td>\n",
       "      <td>10398.113020</td>\n",
       "      <td>-7246.051906</td>\n",
       "      <td>1905.871543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>194</td>\n",
       "      <td>279</td>\n",
       "      <td>185</td>\n",
       "      <td>0.663082</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5661.168296</td>\n",
       "      <td>8149.672960</td>\n",
       "      <td>-6131.230195</td>\n",
       "      <td>4777.647383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>37</td>\n",
       "      <td>258</td>\n",
       "      <td>193</td>\n",
       "      <td>0.748062</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-741.731270</td>\n",
       "      <td>6944.454356</td>\n",
       "      <td>-2608.157388</td>\n",
       "      <td>900.330731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>44</td>\n",
       "      <td>321</td>\n",
       "      <td>84</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-825.538565</td>\n",
       "      <td>5442.020841</td>\n",
       "      <td>-3785.723397</td>\n",
       "      <td>1090.669934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>61</td>\n",
       "      <td>571</td>\n",
       "      <td>150</td>\n",
       "      <td>0.262697</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1410.124373</td>\n",
       "      <td>14905.760188</td>\n",
       "      <td>-7888.573990</td>\n",
       "      <td>2414.717161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>53</td>\n",
       "      <td>386</td>\n",
       "      <td>94</td>\n",
       "      <td>0.243523</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1131.178560</td>\n",
       "      <td>7495.668233</td>\n",
       "      <td>-5146.129776</td>\n",
       "      <td>1419.699789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>40</td>\n",
       "      <td>240</td>\n",
       "      <td>129</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-1062.097073</td>\n",
       "      <td>6777.095667</td>\n",
       "      <td>-4093.124800</td>\n",
       "      <td>1857.609283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>91</td>\n",
       "      <td>491</td>\n",
       "      <td>53</td>\n",
       "      <td>0.107943</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2544.677332</td>\n",
       "      <td>8466.793201</td>\n",
       "      <td>-9718.791726</td>\n",
       "      <td>2322.317592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>52</td>\n",
       "      <td>451</td>\n",
       "      <td>77</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-996.547453</td>\n",
       "      <td>6698.215650</td>\n",
       "      <td>-5274.069872</td>\n",
       "      <td>1133.490981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>57</td>\n",
       "      <td>481</td>\n",
       "      <td>46</td>\n",
       "      <td>0.095634</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-868.226934</td>\n",
       "      <td>3902.283969</td>\n",
       "      <td>-4746.843175</td>\n",
       "      <td>729.700802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>68</td>\n",
       "      <td>552</td>\n",
       "      <td>433</td>\n",
       "      <td>0.784420</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1416.893447</td>\n",
       "      <td>7226.283413</td>\n",
       "      <td>-7220.440219</td>\n",
       "      <td>1506.684666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>25</td>\n",
       "      <td>361</td>\n",
       "      <td>299</td>\n",
       "      <td>0.828255</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-558.424267</td>\n",
       "      <td>6856.129485</td>\n",
       "      <td>-4622.800395</td>\n",
       "      <td>1040.622133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>46</td>\n",
       "      <td>357</td>\n",
       "      <td>294</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-938.161736</td>\n",
       "      <td>5174.051527</td>\n",
       "      <td>-4154.020432</td>\n",
       "      <td>1221.276819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>46</td>\n",
       "      <td>298</td>\n",
       "      <td>171</td>\n",
       "      <td>0.573826</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-962.812862</td>\n",
       "      <td>7610.620556</td>\n",
       "      <td>-3877.675643</td>\n",
       "      <td>1619.438241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>13</td>\n",
       "      <td>297</td>\n",
       "      <td>265</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-231.681050</td>\n",
       "      <td>5602.885776</td>\n",
       "      <td>-2307.366957</td>\n",
       "      <td>400.410836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>32</td>\n",
       "      <td>293</td>\n",
       "      <td>63</td>\n",
       "      <td>0.215017</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-670.285856</td>\n",
       "      <td>7428.371867</td>\n",
       "      <td>-3546.373845</td>\n",
       "      <td>1215.887876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>26</td>\n",
       "      <td>271</td>\n",
       "      <td>81</td>\n",
       "      <td>0.298893</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-492.346588</td>\n",
       "      <td>4268.777744</td>\n",
       "      <td>-2881.183292</td>\n",
       "      <td>651.777334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>54</td>\n",
       "      <td>415</td>\n",
       "      <td>255</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1034.496283</td>\n",
       "      <td>5204.771443</td>\n",
       "      <td>-4658.324936</td>\n",
       "      <td>1150.690357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>81</td>\n",
       "      <td>597</td>\n",
       "      <td>508</td>\n",
       "      <td>0.850921</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2045.279096</td>\n",
       "      <td>8953.846477</td>\n",
       "      <td>-10426.409419</td>\n",
       "      <td>2211.214273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>70</td>\n",
       "      <td>393</td>\n",
       "      <td>51</td>\n",
       "      <td>0.129771</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-2110.381298</td>\n",
       "      <td>12368.483679</td>\n",
       "      <td>-6858.282111</td>\n",
       "      <td>2636.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>51</td>\n",
       "      <td>342</td>\n",
       "      <td>74</td>\n",
       "      <td>0.216374</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1088.399688</td>\n",
       "      <td>6572.750845</td>\n",
       "      <td>-4173.238519</td>\n",
       "      <td>1408.242989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>33</td>\n",
       "      <td>250</td>\n",
       "      <td>217</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-994.313323</td>\n",
       "      <td>12707.433166</td>\n",
       "      <td>-4848.891997</td>\n",
       "      <td>2327.504680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>36</td>\n",
       "      <td>544</td>\n",
       "      <td>255</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-756.328073</td>\n",
       "      <td>5288.014402</td>\n",
       "      <td>-6307.971106</td>\n",
       "      <td>729.284499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>29</td>\n",
       "      <td>270</td>\n",
       "      <td>102</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-549.403759</td>\n",
       "      <td>3711.657858</td>\n",
       "      <td>-2766.584550</td>\n",
       "      <td>798.847723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>32</td>\n",
       "      <td>238</td>\n",
       "      <td>193</td>\n",
       "      <td>0.810924</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-726.372564</td>\n",
       "      <td>9334.300829</td>\n",
       "      <td>-2877.967755</td>\n",
       "      <td>1407.582128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>51</td>\n",
       "      <td>276</td>\n",
       "      <td>217</td>\n",
       "      <td>0.786232</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1241.111651</td>\n",
       "      <td>10058.535181</td>\n",
       "      <td>-4001.084273</td>\n",
       "      <td>2267.690926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>72</td>\n",
       "      <td>507</td>\n",
       "      <td>123</td>\n",
       "      <td>0.242604</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1907.706772</td>\n",
       "      <td>10464.715984</td>\n",
       "      <td>-7914.117555</td>\n",
       "      <td>2368.045582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>24</td>\n",
       "      <td>307</td>\n",
       "      <td>291</td>\n",
       "      <td>0.947883</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-456.958134</td>\n",
       "      <td>5980.766855</td>\n",
       "      <td>-2881.457095</td>\n",
       "      <td>697.504389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>31</td>\n",
       "      <td>252</td>\n",
       "      <td>158</td>\n",
       "      <td>0.626984</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-588.122204</td>\n",
       "      <td>8003.426068</td>\n",
       "      <td>-2974.514113</td>\n",
       "      <td>1026.469142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>103</td>\n",
       "      <td>446</td>\n",
       "      <td>276</td>\n",
       "      <td>0.618834</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2603.375042</td>\n",
       "      <td>6821.037450</td>\n",
       "      <td>-7605.964306</td>\n",
       "      <td>2386.759013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>47</td>\n",
       "      <td>380</td>\n",
       "      <td>352</td>\n",
       "      <td>0.926316</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-924.597451</td>\n",
       "      <td>9050.083608</td>\n",
       "      <td>-4537.784521</td>\n",
       "      <td>1327.901071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>31</td>\n",
       "      <td>351</td>\n",
       "      <td>284</td>\n",
       "      <td>0.809117</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-664.795899</td>\n",
       "      <td>5525.060743</td>\n",
       "      <td>-3756.406448</td>\n",
       "      <td>1031.117560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>29</td>\n",
       "      <td>501</td>\n",
       "      <td>411</td>\n",
       "      <td>0.820359</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-876.114906</td>\n",
       "      <td>17681.508213</td>\n",
       "      <td>-8399.574758</td>\n",
       "      <td>1841.759977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>46</td>\n",
       "      <td>452</td>\n",
       "      <td>97</td>\n",
       "      <td>0.214602</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-1150.321974</td>\n",
       "      <td>9563.581761</td>\n",
       "      <td>-6502.141417</td>\n",
       "      <td>1802.280398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>50</td>\n",
       "      <td>325</td>\n",
       "      <td>26</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-964.569008</td>\n",
       "      <td>8774.595960</td>\n",
       "      <td>-3839.744189</td>\n",
       "      <td>1586.566394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>98</td>\n",
       "      <td>328</td>\n",
       "      <td>284</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2783.863953</td>\n",
       "      <td>6672.771486</td>\n",
       "      <td>-5813.950281</td>\n",
       "      <td>2569.729559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>21</td>\n",
       "      <td>253</td>\n",
       "      <td>219</td>\n",
       "      <td>0.865613</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-405.092550</td>\n",
       "      <td>7921.108455</td>\n",
       "      <td>-2712.062483</td>\n",
       "      <td>802.301801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>59</td>\n",
       "      <td>432</td>\n",
       "      <td>152</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1112.933074</td>\n",
       "      <td>6943.217989</td>\n",
       "      <td>-5055.213671</td>\n",
       "      <td>1393.987444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>56</td>\n",
       "      <td>379</td>\n",
       "      <td>93</td>\n",
       "      <td>0.245383</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1382.168842</td>\n",
       "      <td>12749.442749</td>\n",
       "      <td>-5128.681792</td>\n",
       "      <td>2008.228958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>28</td>\n",
       "      <td>198</td>\n",
       "      <td>123</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-491.882706</td>\n",
       "      <td>3098.293308</td>\n",
       "      <td>-1975.300543</td>\n",
       "      <td>729.299974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>52</td>\n",
       "      <td>417</td>\n",
       "      <td>42</td>\n",
       "      <td>0.100719</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1378.524063</td>\n",
       "      <td>8967.764766</td>\n",
       "      <td>-6376.578387</td>\n",
       "      <td>1856.545255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>50</td>\n",
       "      <td>380</td>\n",
       "      <td>263</td>\n",
       "      <td>0.692105</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1080.758776</td>\n",
       "      <td>8639.294208</td>\n",
       "      <td>-4184.748742</td>\n",
       "      <td>1365.345415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>95</td>\n",
       "      <td>488</td>\n",
       "      <td>444</td>\n",
       "      <td>0.909836</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2161.645637</td>\n",
       "      <td>6865.854096</td>\n",
       "      <td>-6917.663934</td>\n",
       "      <td>2065.090613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>32</td>\n",
       "      <td>280</td>\n",
       "      <td>151</td>\n",
       "      <td>0.539286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-711.296846</td>\n",
       "      <td>5433.975143</td>\n",
       "      <td>-3738.668959</td>\n",
       "      <td>1082.893013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>62</td>\n",
       "      <td>380</td>\n",
       "      <td>54</td>\n",
       "      <td>0.142105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1457.847470</td>\n",
       "      <td>9896.229773</td>\n",
       "      <td>-5637.764806</td>\n",
       "      <td>2333.884578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>33</td>\n",
       "      <td>380</td>\n",
       "      <td>152</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-666.234063</td>\n",
       "      <td>5753.903379</td>\n",
       "      <td>-4568.531120</td>\n",
       "      <td>801.547053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>26</td>\n",
       "      <td>449</td>\n",
       "      <td>86</td>\n",
       "      <td>0.191537</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-498.828819</td>\n",
       "      <td>5654.895658</td>\n",
       "      <td>-4796.714638</td>\n",
       "      <td>689.814855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>51</td>\n",
       "      <td>371</td>\n",
       "      <td>121</td>\n",
       "      <td>0.326146</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1094.831896</td>\n",
       "      <td>4699.868556</td>\n",
       "      <td>-4631.532304</td>\n",
       "      <td>1028.793357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_dataset_size  test_dataset_size  total_correct  total_accuracy  \\\n",
       "101                  23                577            372        0.644714   \n",
       "102                 199                308            149        0.483766   \n",
       "103                  64                528            184        0.348485   \n",
       "104                 194                279            185        0.663082   \n",
       "105                  37                258            193        0.748062   \n",
       "106                  44                321             84        0.261682   \n",
       "107                  61                571            150        0.262697   \n",
       "108                  53                386             94        0.243523   \n",
       "109                  40                240            129        0.537500   \n",
       "110                  91                491             53        0.107943   \n",
       "111                  52                451             77        0.170732   \n",
       "112                  57                481             46        0.095634   \n",
       "113                  68                552            433        0.784420   \n",
       "114                  25                361            299        0.828255   \n",
       "115                  46                357            294        0.823529   \n",
       "116                  46                298            171        0.573826   \n",
       "117                  13                297            265        0.892256   \n",
       "118                  32                293             63        0.215017   \n",
       "119                  26                271             81        0.298893   \n",
       "120                  54                415            255        0.614458   \n",
       "121                  81                597            508        0.850921   \n",
       "122                  70                393             51        0.129771   \n",
       "123                  51                342             74        0.216374   \n",
       "124                  33                250            217        0.868000   \n",
       "125                  36                544            255        0.468750   \n",
       "126                  29                270            102        0.377778   \n",
       "127                  32                238            193        0.810924   \n",
       "128                  51                276            217        0.786232   \n",
       "129                  72                507            123        0.242604   \n",
       "130                  24                307            291        0.947883   \n",
       "131                  31                252            158        0.626984   \n",
       "132                 103                446            276        0.618834   \n",
       "133                  47                380            352        0.926316   \n",
       "134                  31                351            284        0.809117   \n",
       "135                  29                501            411        0.820359   \n",
       "136                  46                452             97        0.214602   \n",
       "137                  50                325             26        0.080000   \n",
       "138                  98                328            284        0.865854   \n",
       "139                  21                253            219        0.865613   \n",
       "140                  59                432            152        0.351852   \n",
       "141                  56                379             93        0.245383   \n",
       "142                  28                198            123        0.621212   \n",
       "143                  52                417             42        0.100719   \n",
       "144                  50                380            263        0.692105   \n",
       "145                  95                488            444        0.909836   \n",
       "146                  32                280            151        0.539286   \n",
       "147                  62                380             54        0.142105   \n",
       "148                  33                380            152        0.400000   \n",
       "149                  26                449             86        0.191537   \n",
       "150                  51                371            121        0.326146   \n",
       "\n",
       "     top10_correct  top10_accuracy  train_log_likelihood  train_perplexity  \\\n",
       "101              8             0.8           -515.169364       6615.645742   \n",
       "102              0             0.0          -5799.870234       8396.555199   \n",
       "103              2             0.2          -1450.326732      10398.113020   \n",
       "104              0             0.0          -5661.168296       8149.672960   \n",
       "105              0             0.0           -741.731270       6944.454356   \n",
       "106              1             0.1           -825.538565       5442.020841   \n",
       "107              0             0.0          -1410.124373      14905.760188   \n",
       "108              2             0.2          -1131.178560       7495.668233   \n",
       "109              4             0.4          -1062.097073       6777.095667   \n",
       "110              0             0.0          -2544.677332       8466.793201   \n",
       "111              0             0.0           -996.547453       6698.215650   \n",
       "112              1             0.1           -868.226934       3902.283969   \n",
       "113              0             0.0          -1416.893447       7226.283413   \n",
       "114              0             0.0           -558.424267       6856.129485   \n",
       "115              0             0.0           -938.161736       5174.051527   \n",
       "116              4             0.4           -962.812862       7610.620556   \n",
       "117              1             0.1           -231.681050       5602.885776   \n",
       "118              0             0.0           -670.285856       7428.371867   \n",
       "119              3             0.3           -492.346588       4268.777744   \n",
       "120              0             0.0          -1034.496283       5204.771443   \n",
       "121              1             0.1          -2045.279096       8953.846477   \n",
       "122              4             0.4          -2110.381298      12368.483679   \n",
       "123              2             0.2          -1088.399688       6572.750845   \n",
       "124              0             0.0           -994.313323      12707.433166   \n",
       "125              4             0.4           -756.328073       5288.014402   \n",
       "126              8             0.8           -549.403759       3711.657858   \n",
       "127              0             0.0           -726.372564       9334.300829   \n",
       "128              0             0.0          -1241.111651      10058.535181   \n",
       "129              1             0.1          -1907.706772      10464.715984   \n",
       "130              0             0.0           -456.958134       5980.766855   \n",
       "131              0             0.0           -588.122204       8003.426068   \n",
       "132              0             0.0          -2603.375042       6821.037450   \n",
       "133              8             0.8           -924.597451       9050.083608   \n",
       "134              0             0.0           -664.795899       5525.060743   \n",
       "135              8             0.8           -876.114906      17681.508213   \n",
       "136              3             0.3          -1150.321974       9563.581761   \n",
       "137              1             0.1           -964.569008       8774.595960   \n",
       "138              0             0.0          -2783.863953       6672.771486   \n",
       "139              0             0.0           -405.092550       7921.108455   \n",
       "140              2             0.2          -1112.933074       6943.217989   \n",
       "141              5             0.5          -1382.168842      12749.442749   \n",
       "142              0             0.0           -491.882706       3098.293308   \n",
       "143              0             0.0          -1378.524063       8967.764766   \n",
       "144              0             0.0          -1080.758776       8639.294208   \n",
       "145              0             0.0          -2161.645637       6865.854096   \n",
       "146              0             0.0           -711.296846       5433.975143   \n",
       "147              1             0.1          -1457.847470       9896.229773   \n",
       "148              2             0.2           -666.234063       5753.903379   \n",
       "149              0             0.0           -498.828819       5654.895658   \n",
       "150              0             0.0          -1094.831896       4699.868556   \n",
       "\n",
       "     test_log_likelihood  test_perplexity  \n",
       "101         -5862.518816       633.167644  \n",
       "102         -6823.307163      5272.106013  \n",
       "103         -7246.051906      1905.871543  \n",
       "104         -6131.230195      4777.647383  \n",
       "105         -2608.157388       900.330731  \n",
       "106         -3785.723397      1090.669934  \n",
       "107         -7888.573990      2414.717161  \n",
       "108         -5146.129776      1419.699789  \n",
       "109         -4093.124800      1857.609283  \n",
       "110         -9718.791726      2322.317592  \n",
       "111         -5274.069872      1133.490981  \n",
       "112         -4746.843175       729.700802  \n",
       "113         -7220.440219      1506.684666  \n",
       "114         -4622.800395      1040.622133  \n",
       "115         -4154.020432      1221.276819  \n",
       "116         -3877.675643      1619.438241  \n",
       "117         -2307.366957       400.410836  \n",
       "118         -3546.373845      1215.887876  \n",
       "119         -2881.183292       651.777334  \n",
       "120         -4658.324936      1150.690357  \n",
       "121        -10426.409419      2211.214273  \n",
       "122         -6858.282111      2636.591100  \n",
       "123         -4173.238519      1408.242989  \n",
       "124         -4848.891997      2327.504680  \n",
       "125         -6307.971106       729.284499  \n",
       "126         -2766.584550       798.847723  \n",
       "127         -2877.967755      1407.582128  \n",
       "128         -4001.084273      2267.690926  \n",
       "129         -7914.117555      2368.045582  \n",
       "130         -2881.457095       697.504389  \n",
       "131         -2974.514113      1026.469142  \n",
       "132         -7605.964306      2386.759013  \n",
       "133         -4537.784521      1327.901071  \n",
       "134         -3756.406448      1031.117560  \n",
       "135         -8399.574758      1841.759977  \n",
       "136         -6502.141417      1802.280398  \n",
       "137         -3839.744189      1586.566394  \n",
       "138         -5813.950281      2569.729559  \n",
       "139         -2712.062483       802.301801  \n",
       "140         -5055.213671      1393.987444  \n",
       "141         -5128.681792      2008.228958  \n",
       "142         -1975.300543       729.299974  \n",
       "143         -6376.578387      1856.545255  \n",
       "144         -4184.748742      1365.345415  \n",
       "145         -6917.663934      2065.090613  \n",
       "146         -3738.668959      1082.893013  \n",
       "147         -5637.764806      2333.884578  \n",
       "148         -4568.531120       801.547053  \n",
       "149         -4796.714638       689.814855  \n",
       "150         -4631.532304      1028.793357  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = []\n",
    "for x in range(101, 151):\n",
    "    users.append(x)\n",
    "    \n",
    "df = pd.DataFrame(index=users)\n",
    "df['train_dataset_size'] = train_size\n",
    "df['test_dataset_size'] = test_size\n",
    "df['total_correct'] = final_result\n",
    "df['total_accuracy'] = final_result_accuracy\n",
    "df['top10_correct'] = final_result2\n",
    "df['top10_accuracy'] = final_result2_accuracy\n",
    "df['train_log_likelihood'] = train_log_likelihood\n",
    "df['train_perplexity'] = train_perplexity\n",
    "df['test_log_likelihood'] = test_log_likelihood\n",
    "df['test_perplexity'] = test_perplexity\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee11dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=r'C:\\Users\\sumsu\\Desktop\\QUT\\sem2\\IFN712\\project\\RCV1 Dataset'\n",
    "# df.to_csv(path+'countvectorizer_result.csv')\n",
    "df.to_csv('tfidfvectorizer_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd4320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
